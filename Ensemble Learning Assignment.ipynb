{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5be0db",
   "metadata": {},
   "source": [
    "**Question 1:  What is Ensemble Learning in machine learning? Explain the key idea \n",
    "behind it.**\n",
    "\n",
    "Ensemble Learning in machine learning is a technique where multiple models (often called weak learners or base models) are trained and then combined to solve the same problem. Instead of relying on a single model, ensemble methods aggregate the predictions of several models to produce a more accurate, robust, and generalizable output.\n",
    "\n",
    "#### Key Idea Behind Ensemble Learning\n",
    "\n",
    "Ensemble Learning in machine learning is a technique where multiple models (often called weak learners or base models) are trained and then combined to solve the same problem. Instead of relying on a single model, ensemble methods aggregate the predictions of several models to produce a more accurate, robust, and generalizable output.The core concept behind ensemble learning is based on the \"wisdom of crowds\" principle. Just as a group of people might collectively make better decisions than individuals, multiple machine learning models can compensate for each other's weaknesses and reduce overall prediction errors.\n",
    "\n",
    "#### How It Works\n",
    "1. Training multiple models - Different algorithms or the same algorithm with different parameters/training data\n",
    "2. Combining their predictions - Using techniques like voting, averaging, or weighted combinations\n",
    "3. Producing a final prediction - That's typically more robust and accurate than individual models\n",
    "\n",
    "#### Why Ensembles Are Effective\n",
    "Ensembles improve performance through several mechanisms:\n",
    "- Error reduction: Individual model errors often cancel out when combined\n",
    "- Variance reduction: Averaging multiple models reduces overfitting\n",
    "- Bias reduction: Different models may capture different aspects of the data\n",
    "- Increased robustness: Less likely to fail catastrophically on edge cases\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab6065",
   "metadata": {},
   "source": [
    "**Question 2: What is the difference between Bagging and Boosting?**\n",
    "\n",
    "Both Bagging and Boosting are popular ensemble learning techniques, but they work in different ways. \n",
    "Let’s break it down clearly:\n",
    "\n",
    "**What is Bagging?**\n",
    "\n",
    "- Bagging, short for Bootstrap Aggregating, is an ensemble method designed to reduce the variance of machine learning models. Bagging involves training multiple independent models on different subsets of the dataset and then combining their predictions to produce a final output. This method helps improve model performance by reducing overfitting and creating a more stable model.\n",
    "\n",
    "**What is Boosting?**\n",
    "\n",
    "- Boosting is another powerful ensemble technique in machine learning, but unlike Bagging, Boosting focuses on reducing bias rather than variance. The main idea behind Boosting is to train models sequentially, where each new model tries to correct the errors made by the previous models. Boosting creates a strong model by combining the predictions of weak learners (models that are only slightly better than random guessing), improving accuracy and performance over time.\n",
    "\n",
    "### Differences Between Bagging and Boosting:\n",
    "\n",
    "**Sequential vs. Parallel:**\n",
    "- Bagging: The base learners are trained independently in parallel, as each learner works on a different subset of the data. The final prediction is typically an average or vote of all base learners.\n",
    "- Boosting: The base learners are trained sequentially, and each learner focuses on correcting the mistakes of its predecessors. The final prediction is a weighted sum of the individual learner predictions.\n",
    "\n",
    "**Data Sampling:**\n",
    "- Bagging: Utilizes bootstrapping to create multiple subsets of the training data, allowing for variations in the training sets for each base learner.\n",
    "- Boosting: Assigns weights to instances in the training set, with higher weights given to misclassified instances to guide subsequent learners.\n",
    "\n",
    "**Weighting of Base Learners:**\n",
    "- Bagging: All base learners typically have equal weight when making the final prediction.\n",
    "- Boosting: Assigns different weights to each base learner based on its performance, giving more influence to learners that perform well on challenging instances.\n",
    "\n",
    "**Handling Noisy Data and Outliers:**\n",
    "- Bagging: Robust to noisy data and outliers due to the averaging or voting mechanism, which reduces the impact of individual errors.\n",
    "- Boosting: More sensitive to noisy data and outliers, as the focus on misclassified instances might lead to overfitting on these instances.\n",
    "\n",
    "**Model Diversity:**\n",
    "- Bagging: Aims to create diverse base learners through random subsets of the data and, in the case of Random Forests, random feature selection for each tree.\n",
    "- Boosting: Focuses on improving the performance of weak learners sequentially, with each learner addressing the weaknesses of its predecessors.\n",
    "\n",
    "**Bias and Variance:**\n",
    "- Bagging: Primarily reduces variance by averaging predictions from multiple models, making it effective for models with high variance.\n",
    "- Boosting: Addresses both bias and variance, with a focus on reducing bias by sequentially correcting mistakes made by weak learners.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a869252",
   "metadata": {},
   "source": [
    "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods \n",
    "like Random Forest?**\n",
    "\n",
    "Bootstrap sampling or Bootstrapping is a statistical procedure that resamples a single data set to create many simulated samples. This process allows for the calculation of standard errors, confidence intervals, and hypothesis testing,” according to a post on bootstrapping statistics from statistician Jim Frost. Bootstrapping is a resampling technique used to estimate population statistics by sampling from a dataset with replacement. It can be used to estimate summary statistics such as the mean and standard deviation. It is used in applied machine learning to estimate the quality of a machine learning model at predicting data that is not included in the training data.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "- Each bootstrap sample has the same size as the original dataset\n",
    "- Typically, each bootstrap sample contains about 63.2% of unique observations from the original data\n",
    "- About 36.8% of original observations are left out (called \"out-of-bag\" samples)\n",
    "\n",
    "- Bootstrap sampling is a statistical resampling technique that creates multiple datasets from a single original dataset by sampling with replacement. It's named after the phrase \"pulling oneself up by one's bootstraps\" because it generates new samples from existing data.\n",
    "\n",
    "### Role in Bagging (Random Forest)\n",
    "\n",
    "**Diversity Creation:**\n",
    "\n",
    "- Bagging trains multiple models on different bootstrap samples.\n",
    "\n",
    "- Since each sample is slightly different, the models learn different patterns.\n",
    "\n",
    "**Reduces Variance:**\n",
    "\n",
    "- Individual decision trees can overfit the training data.\n",
    "\n",
    "- By averaging predictions from many trees trained on different bootstrap samples, Bagging smooths out noise and reduces variance.\n",
    "\n",
    "**Out-of-Bag (OOB) Error Estimation:**\n",
    "\n",
    "- Since each bootstrap sample leaves out about 36% of the data (on average), those left-out samples can be used as a validation set to estimate error without needing a separate test dataset.\n",
    "\n",
    "- This is called OOB error, often used in Random Forests.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0296a",
   "metadata": {},
   "source": [
    "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
    "\n",
    "Out-of-Bag (OOB) samples are the observations that are not selected during bootstrap sampling for training individual models in an ensemble. They serve as a built-in validation set that provides an elegant way to evaluate ensemble performance without requiring separate holdout data.\n",
    "\n",
    "- In bootstrap sampling, each new dataset is created by sampling with replacement from the original dataset.\n",
    "- On average, about 63% of the data points are selected in each bootstrap sample.\n",
    "- The remaining ~37% of the data points are not selected → these are called Out-of-Bag (OOB) samples.\n",
    "\n",
    "**Example:**\n",
    "- Original dataset = [A, B, C, D, E]\n",
    "- Bootstrap sample (for one tree) = [B, E, A, B, D]\n",
    "- OOB samples = [C] (not included in this bootstrap).\n",
    "\n",
    "OOB (out-of-bag) score is a performance metric for a machine learning model, specifically for ensemble models such as random forests. It is calculated using the samples that are not used in the training of the model, which is called out-of-bag samples.\n",
    "\n",
    "\n",
    "### How is the OOB Score Used?\n",
    "\n",
    "The OOB score is calculated by using these unused samples to test the performance of the ensemble. The process works as follows:\n",
    "\n",
    "1.  For each data point in the original dataset, identify all the individual base models (e.g., decision trees) for which this data point was an OOB sample.\n",
    "2.  Have those specific models make a prediction for that data point.\n",
    "3.  Combine these predictions (e.g., by voting for classification or averaging for regression) to get an ensemble prediction for that single data point.\n",
    "4.  Compare this prediction to the actual label of the data point.\n",
    "5.  Repeat this process for every data point in the original dataset and calculate an overall performance metric (e.g., accuracy, mean squared error, etc.).\n",
    "\n",
    "This resulting OOB score provides a robust and unbiased estimate of the model's generalization performance. It's similar to a cross-validation score but is calculated for free as part of the bagging process, eliminating the need to set aside a separate validation set. This allows the model to be trained on the maximum amount of data while still providing a reliable performance estimate.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b554f1",
   "metadata": {},
   "source": [
    "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
    "\n",
    "\n",
    "#### 1.Single Decision Tree\n",
    "\n",
    "**How it’s calculated:**\n",
    "\n",
    "- At each split, the tree chooses the feature that gives the highest reduction in impurity (e.g., Gini impurity, entropy for classification, or variance for regression).\n",
    "\n",
    "- The feature importance is then computed by summing up the impurity reduction contributed by that feature across all splits in the tree.\n",
    "\n",
    "- The values are normalized so they sum to 1.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Instability: A small change in data can lead to a very different tree → and hence different feature importance.\n",
    "\n",
    "- Bias toward high-cardinality features: Features with many unique values (e.g., ID-like features) may appear artificially more important.\n",
    "\n",
    "- Reflects the decision path of one specific tree, so it may not represent the overall dataset well.\n",
    "\n",
    "#### 2. Random Forest\n",
    "\n",
    "**How it’s calculated:**\n",
    "\n",
    "- Each tree in the forest gives feature importance (same method as above).\n",
    "\n",
    "- The importances are then averaged across all trees, giving a more stable and reliable measure.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Robustness: Since Random Forest uses bootstrap sampling and random feature selection, the importance values are averaged over many trees → reducing bias from any single tree.\n",
    "\n",
    "- More generalizable: Captures feature relevance across many possible decision boundaries.\n",
    "\n",
    "- Less prone to overfitting compared to a single tree.\n",
    "\n",
    "\n",
    "\n",
    "#### Feature Importance: Decision Tree vs Random Forest\n",
    "\n",
    "**1. Calculation Method**\n",
    "- Decision Tree → Looks at how much each feature reduces impurity (like Gini or entropy) when the tree splits. The total reduction for a feature is summed up within that single tree.\n",
    "- Random Forest → Builds many trees, each with its own importance values, and then averages them. This way, the final feature importance reflects many different trees, not just one.\n",
    "\n",
    "**2. Stability**\n",
    "- Decision Tree → Very unstable. If you change the dataset a little (like add/remove a few rows), the splits in the tree may change a lot, which can completely change the importance ranking.\n",
    "- Random Forest → Much more stable. Since it combines results from many trees trained on different subsets, small changes in data don’t affect the overall importance values much.\n",
    "\n",
    "**3. Bias**\n",
    "- Decision Tree → Often biased toward features with many unique values (e.g., ID numbers, continuous variables). Such features can split the data more ways, so the tree may overestimate their importance.\n",
    "- Random Forest → This bias is reduced because the forest averages importance across many trees and uses random subsets of features at each split. Still not perfect, but more reliable.\n",
    "\n",
    "**4. Generalization**\n",
    "- Decision Tree → Only shows what mattered for that one tree’s decision path. It might not represent the overall patterns in the data well.\n",
    "- Random Forest → Captures feature importance more broadly, since it summarizes patterns found by many trees. This makes it a better reflection of which features are truly useful across the dataset.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42c1aa",
   "metadata": {},
   "source": [
    "**Question 6: Write a Python program to:** \n",
    "- Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() \n",
    "- Train a Random Forest Classifier \n",
    "- Print the top 5 most important features based on feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05070d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "==================================================\n",
      "1. worst area: 0.1394\n",
      "2. worst concave points: 0.1322\n",
      "3. mean concave points: 0.1070\n",
      "4. worst radius: 0.0828\n",
      "5. worst perimeter: 0.0808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# 2. Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Get feature importance scores\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 4. Create a DataFrame for better readability\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# 5. Sort by importance in descending order\n",
    "importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# 6.Print top 5 most important features\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(5):\n",
    "    feature = importance_df.iloc[i]['Feature']\n",
    "    score = importance_df.iloc[i]['Importance']\n",
    "    print(f\"{i+1}. {feature}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4d742",
   "metadata": {},
   "source": [
    "**Question 7: Write a Python program to:**\n",
    "- Train a Bagging Classifier using Decision Trees on the Iris dataset \n",
    "- Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5e9a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison:\n",
      "Single Decision Tree Accuracy: 0.9667\n",
      "Bagging Classifier Accuracy: 0.9833\n",
      "Improvement: 0.0167\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train Single Decision Tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train Bagging Classifier with Decision Trees\n",
    "bagging_classifier = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "single_tree_pred = single_tree.predict(X_test)\n",
    "bagging_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "single_tree_accuracy = accuracy_score(y_test, single_tree_pred)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Single Decision Tree Accuracy: {single_tree_accuracy:.4f}\")\n",
    "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"Improvement: {bagging_accuracy - single_tree_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86cdca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Single Decision Tree: 0.93\n",
      "Accuracy of Bagging Classifier: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 2. Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# 4. Train a Bagging Classifier with Decision Trees as base estimator\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,   # number of trees in the ensemble\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# 5. Print accuracies\n",
    "print(\"Accuracy of Single Decision Tree: {:.2f}\".format(dt_accuracy))\n",
    "print(\"Accuracy of Bagging Classifier: {:.2f}\".format(bagging_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ee81e",
   "metadata": {},
   "source": [
    "**Question 8: Write a Python program to:** \n",
    "- Train a Random Forest Classifier \n",
    "- Tune hyperparameters max_depth and n_estimators using GridSearchCV \n",
    "- Print the best parameters and final accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4e56f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearchCV...\n",
      "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
      "Final Accuracy on Test Set: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset (Iris for simplicity)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 2. Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Define Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 4. Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],   # number of trees\n",
    "    'max_depth': [None, 3, 5, 7]      # tree depth\n",
    "}\n",
    "\n",
    "# 5. Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1          # use all CPU cores\n",
    ")\n",
    "\n",
    "# 6. Fit GridSearchCV\n",
    "print(\"Performing GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Get best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 8. Predict on test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 9. Print results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Final Accuracy on Test Set: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d33127",
   "metadata": {},
   "source": [
    "**Question 9: Write a Python program to:** \n",
    "- Train a Bagging Regressor and a Random Forest Regressor on the California \n",
    "Housing dataset \n",
    "- Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ab7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Performance Comparison:\n",
      "Bagging Regressor MSE: 0.2568\n",
      "Random Forest Regressor MSE: 0.2565\n",
      "Difference (Bagging - RF): 0.0003\n",
      "Random Forest performs better!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Bagging Regressor with Decision Trees\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "bagging_pred = bagging_regressor.predict(X_test)\n",
    "rf_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Errors\n",
    "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Regression Performance Comparison:\")\n",
    "print(f\"Bagging Regressor MSE: {bagging_mse:.4f}\")\n",
    "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
    "print(f\"Difference (Bagging - RF): {bagging_mse - rf_mse:.4f}\")\n",
    "\n",
    "if rf_mse < bagging_mse:\n",
    "    print(\"Random Forest performs better!\")\n",
    "else:\n",
    "    print(\"Bagging Regressor performs better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d51e9c",
   "metadata": {},
   "source": [
    "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. \n",
    "You decide to use ensemble techniques to increase model performance.**\n",
    "\n",
    "Explain your step-by-step approach to: \n",
    "- Choose between Bagging or Boosting \n",
    "- Handle overfitting \n",
    "- Select base models \n",
    "- Evaluate performance using cross-validation \n",
    "- Justify how ensemble learning improves decision-making in this real-world context.\n",
    "\n",
    "### Choice of Ensemble Technique: Bagging vs. Boosting\n",
    "For predicting loan defaults, boosting is the better choice. While bagging is good at reducing variance, boosting does a great job of reducing bias and creating accurate models based on a series of weak learners. This sequential method helps the model learn complex patterns in customer data and pay attention to the hardest cases. This focus is essential for such an important issue. I would specifically choose a powerful boosting algorithm like XGBoost or LightGBM.\n",
    "\n",
    "### Handling Overfitting\n",
    "Boosting models are effective but can overfit. I would use various strategies to ensure the model works well with new data:\n",
    "\n",
    "1. **Regularization:** I would apply L1 and L2 regularization to discourage complex models.\n",
    "2. **Early Stopping:** I would keep an eye on the model’s performance on a validation set and stop training when performance stops improving. This is an effective way to stop the model from memorizing the training data.\n",
    "3. **Hyperparameter Tuning:** I would carefully adjust key parameters. A smaller learning rate along with a larger number of estimators (trees) usually improves generalization. Limiting the maximum depth of the trees also helps stop overfitting.\n",
    "\n",
    "### Selection of Base Models\n",
    "The best and most common base model for boosting is a shallow decision tree, also known as a \"weak learner.\" These simple models aren’t strong enough to overfit on their own. The boosting algorithm combines the predictions of thousands of these weak learners into one highly accurate final model.\n",
    "\n",
    "### Evaluation Using Cross-Validation\n",
    "To make sure the model is strong and not just the result of a lucky data split, I would use stratified k-fold cross-validation. This method splits the data into k folds, making sure each fold has the same ratio of defaulting and non-defaulting customers. I would look at the average performance across all k folds to get a dependable estimate of how the model will act in the real world.\n",
    "\n",
    "When assessing the model, it’s important to think about the trade-off between precision and recall.\n",
    "* **High Recall** is important to catch as many actual defaulters as possible, reducing the financial risk from missed defaults.\n",
    "* **High Precision** is also crucial to prevent wrongly labeling creditworthy customers as high-risk, which could lead to lost business.\n",
    "To balance these competing risks, I’d monitor a mix of metrics, including ROC-AUC, precision, recall, and the F1-Score.\n",
    "\n",
    "### How Ensemble Learning Improves Decision-Making\n",
    "Ensemble learning, especially boosting, offers great benefits for a financial institution.\n",
    "\n",
    "* **Improved Accuracy & Risk Mitigation:** By combining multiple models, the ensemble can detect complex patterns and provide more accurate predictions of loan defaults. This helps the institution make better choices, which lowers financial risk by approving only those who are creditworthy.\n",
    "* **Enhanced Interpretability:** Boosting models, particularly XGBoost, can show feature importance scores. This helps the institution understand why a customer was marked as high-risk, which is vital for compliance and for offering clear, data-backed reasons for loan decisions.\n",
    "* **Increased Confidence:** Using cross-validation gives a reliable performance estimate, providing decision-makers with strong confidence in the model’s predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
