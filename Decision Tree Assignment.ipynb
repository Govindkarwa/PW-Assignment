{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a6f73",
   "metadata": {},
   "source": [
    "**Question 1:  What is a Decision Tree, and how does it work in the context of \n",
    "classification?**\n",
    "\n",
    "A Decision Tree helps us to make decisions by mapping out different choices and their possible outcomes. It’s used in machine learning for tasks like classification and prediction.It helps us make decisions by showing different options and how they are related. It has a tree-like structure that starts with one main question called the root node which represents the entire dataset. From there, the tree branches out into different possibilities based on features in the data.\n",
    "\n",
    "- Root Node: Starting point representing the whole dataset.\n",
    "- Branches: Lines connecting nodes showing the flow from one decision to another.\n",
    "- Internal Nodes: Points where decisions are made based on data features.\n",
    "- Leaf Nodes: End points of the tree where the final decision or prediction is made.\n",
    "\n",
    "A Decision Tree also helps with decision-making by showing possible outcomes clearly. By looking at the \"branches\" we can quickly compare options and figure out the best choice.\n",
    "\n",
    "**How Decision Trees Work?**\n",
    "1. Start with the Root Node: It begins with a main question at the root node which is derived from the dataset’s features.\n",
    "\n",
    "2. Ask Yes/No Questions: From the root, the tree asks a series of yes/no questions to split the data into subsets based on specific attributes.\n",
    "\n",
    "3. Branching Based on Answers: Each question leads to different branches:\n",
    "\n",
    "If the answer is yes, the tree follows one path.\n",
    "If the answer is no, the tree follows another path.\n",
    "4. Continue Splitting: This branching continues through further decisions helps in reducing the data down step-by-step.\n",
    "\n",
    "5. Reach the Leaf Node: The process ends when there are no more useful questions to ask leading to the leaf node where the final decision or prediction is made.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "1. In the morning: It asks “Tired?”\n",
    "\n",
    "If yes, the tree suggests drinking coffee.\n",
    "If no, it says no coffee is needed.\n",
    "2. In the afternoon: It asks again “Tired?”\n",
    "\n",
    "If yes, it suggests drinking coffee.\n",
    "If no, no coffee is needed.\n",
    "\n",
    "\n",
    "**How does it work in Classification?**\n",
    "\n",
    "Start at the root node: The algorithm looks at all the features and selects the best feature to split on.\n",
    "\n",
    "The \"best feature\" is chosen using criteria like:\n",
    "\n",
    "Gini Impurity (CART algorithm)\n",
    "\n",
    "Information Gain / Entropy (ID3, C4.5 algorithms)\n",
    "\n",
    "Split the dataset: Based on the chosen feature, the dataset is split into subsets.\n",
    "Example: If splitting on Age < 30, the dataset is divided into two groups:\n",
    "\n",
    "Group 1: Age < 30\n",
    "\n",
    "Group 2: Age ≥ 30\n",
    "\n",
    "Repeat recursively: For each subset, the process repeats: select the next best feature and split again.\n",
    "\n",
    "Stop condition: The recursion stops when:\n",
    "\n",
    "All samples in a node belong to the same class.\n",
    "\n",
    "No further improvement is possible (e.g., max depth reached or too few samples).\n",
    "\n",
    "Prediction:\n",
    "\n",
    "To classify a new data point, start at the root node and follow the decision rules until you reach a leaf node.\n",
    "\n",
    "The class label in the leaf node is assigned as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f790783",
   "metadata": {},
   "source": [
    "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. \n",
    "How do they impact the splits in a Decision Tree?**\n",
    "\n",
    "Gini Impurity and Entropy are two fundamental metrics used to measure how \"mixed\" or \"impure\" a node is in a decision tree. They help the tree decide where to split the data to best separate the classes.\n",
    "\n",
    "**Understanding Impurity**\n",
    "Impurity measures how mixed the classes are in a dataset. A \"pure\" node contains samples from only one class, while an \"impure\" node contains a mixture of different classes.\n",
    "\n",
    "**Gini Impurity**\n",
    "\n",
    "Gini Impurity measures the probability of incorrectly classifying a randomly chosen sample if it were labeled according to the class distribution in the node.\n",
    "\n",
    "Formula: Gini = 1 - Σ(pi)²\n",
    "\n",
    "**Properties:**\n",
    "- Range: 0(pure) to 0.5 (for 2 classes, max impurity)\n",
    "- Minimum (0): Pure node (all samples belong to one class)\n",
    "- A node is pure (Gini = 0) if all samples belong to one class.\n",
    "- Lower Gini = better split.\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "Entropy comes from information theory and measures the amount of uncertainty or disorder in the data.\n",
    "\n",
    "Formula: Entropy = -Σ(pi × log₂(pi))\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Range: [0, log₂(k)] where k is the number of classes\n",
    "- Minimum (0): Pure node\n",
    "- Higher entropy = more mixed classes = more disorder\n",
    "\n",
    "**How they impact splits in Decision Trees**\n",
    "\n",
    "- At each node, the algorithm tests all possible features and thresholds.\n",
    "\n",
    "- For each candidate split, it computes the weighted average impurity of child nodes.\n",
    "\n",
    "- The split that minimizes impurity (Gini or Entropy) is chosen.\n",
    "\n",
    "**Differences in practice:**\n",
    "\n",
    "Computational Efficiency\n",
    "\n",
    "- Gini: Faster to compute (no logarithms)\n",
    "- Entropy: More computationally expensive\n",
    "\n",
    "Sensitivity to Class Distribution\n",
    "\n",
    "- Gini: Tends to favor larger partitions and is less sensitive to small changes\n",
    "- Entropy: More sensitive to changes in class probabilities\n",
    "\n",
    "Tree Structure Impact\n",
    "\n",
    "- Gini: Often produces more balanced trees\n",
    "- Entropy: May create slightly deeper trees with more precise splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17a51e",
   "metadata": {},
   "source": [
    "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision \n",
    "Trees? Give one practical advantage of using each.**\n",
    "\n",
    "Pre-pruning and post-pruning are two different strategies to prevent overfitting in decision trees by controlling tree complexity.\n",
    "\n",
    "1. Pre-Pruning (Early Stopping)\n",
    "Pre-pruning, also known as early stopping, involves halting the growth of the decision tree before it becomes fully developed. In this approach, we impose certain conditions (constraints) to stop the tree from growing beyond a certain depth or complexity. This way, the tree is restricted from learning the noise in the training data.\n",
    "\n",
    "2. Post-Pruning (Pruning After Full Growth)\n",
    "Post-pruning, also known as cost-complexity pruning, is the process of growing the decision tree fully, allowing it to overfit the training data, and then trimming back unnecessary branches that do not contribute to improving accuracy on unseen data.\n",
    "\n",
    "**Key Differences**\n",
    "\n",
    "Timing and Process\n",
    "\n",
    "- Pre-Pruning: Applied during tree construction - prevents splits from happening\n",
    "- Post-Pruning: Applied after tree construction - removes already created branches\n",
    "\n",
    "Tree Building Strategy\n",
    "\n",
    "- Pre-Pruning: Conservative approach - stops early to avoid complexity\n",
    "- Post-Pruning: Aggressive approach - builds full tree then selects best structure\n",
    "\n",
    "Computational Requirements\n",
    "\n",
    "- Pre-Pruning: Lower computational cost - builds smaller trees\n",
    "- Post-Pruning: Higher computational cost - builds full tree then prunes\n",
    "\n",
    "Solution Space Exploration\n",
    "\n",
    "- Pre-Pruning: Limited exploration - may miss optimal splits deeper in tree\n",
    "- Post-Pruning: Complete exploration - examines all possible tree structures\n",
    "\n",
    "Implementation Complexity\n",
    "\n",
    "- Pre-Pruning: Simpler to implement - just add stopping conditions\n",
    "- Post-Pruning: More complex - requires pruning algorithms and validation techniques\n",
    "\n",
    "**Practical Advantages**\n",
    "\n",
    "**Pre-Pruning: Computational Efficiency**\n",
    "- **Advantage:** Significantly faster training and lower memory usage\n",
    "- **Real-world application:** E-commerce recommendation systems that need to retrain models frequently as new products and user behaviors emerge. Pre-pruning ensures models can be updated in real-time without system delays.\n",
    "\n",
    "**Post-Pruning: Superior Model Quality**\n",
    "- **Advantage:** Often achieves better generalization and finds optimal tree structures\n",
    "- **Real-world application:** Medical diagnosis systems where accuracy is critical. Post-pruning can discover complex symptom combinations that might be missed by early stopping, potentially identifying rare but important diagnostic patterns that save lives.\n",
    "\n",
    "**When to Use Which**\n",
    "Use Pre-Pruning when:\n",
    "\n",
    "- Computational resources are limited\n",
    "- Training time is critical\n",
    "- Dataset is very large\n",
    "- Reasonable performance is sufficient\n",
    "\n",
    "Use Post-Pruning when:\n",
    "\n",
    "- Maximum accuracy is priority\n",
    "- Computational resources are available\n",
    "- Dataset size is manageable\n",
    "- You need the best possible model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab573",
   "metadata": {},
   "source": [
    "**Question 4: What is Information Gain in Decision Trees, and why is it important for \n",
    "choosing the best split?**\n",
    "\n",
    "Information Gain is a criterion used to evaluate how well a particular feature (attribute) can separate the data into classes. It measures the reduction in entropy — a concept from information theory — after a dataset is split on an attribute. The ability of a decision tree to classify data accurately hinges on the selection of attributes that provide the maximum Information Gain.It quantifies how much \"information\" or \"knowledge\" we gain about the class labels by making a specific split.\n",
    "\n",
    "**Mathematical Formula**\n",
    "\n",
    "Information Gain = Entropy(Parent) - Weighted Average Entropy(Children)\n",
    "\n",
    "More formally: IG(S, A) = Entropy(S) - Σ(|Sv|/|S| × Entropy(Sv))\n",
    "\n",
    "- S = parent dataset\n",
    "- A = attribute/feature being considered for split\n",
    "- Sv = subset of S where attribute A has value v\n",
    "- |S| = number of samples in dataset S\n",
    "\n",
    "**Why Information Gain is Important for Split Selection**\n",
    "\n",
    "**1. Quantitative Split Comparison:** \n",
    "Information Gain provides a numerical value to compare different possible splits objectively. The algorithm can evaluate all features and thresholds, then select the one with the highest information gain.\n",
    "\n",
    "**2. Maximizes Class Separation:** \n",
    "Higher information gain means the split creates more homogeneous child nodes. \n",
    "\n",
    "This leads to:\n",
    "\n",
    "- Purer leaf nodes\n",
    "- Better classification accuracy\n",
    "- Clearer decision boundaries\n",
    "\n",
    "**3. Greedy Optimization Strategy:** \n",
    "At each node, the algorithm greedily chooses the split that provides maximum immediate information gain, leading to locally optimal decisions that generally result in good global performance.\n",
    "\n",
    "**4. Handles Multiple Data Types:** \n",
    "\n",
    "Information gain works effectively with:\n",
    "\n",
    "- Categorical features: Direct entropy calculation for each category\n",
    "- Numerical features: Tests various threshold values\n",
    "- Mixed datasets: Uniform approach across different feature types\n",
    "\n",
    "Information Gain is the reduction in entropy achieved by splitting on a feature. It is important because the feature with the highest Information Gain is chosen at each step of building the decision tree, ensuring that the data is divided into the purest subsets and leading to a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa141924",
   "metadata": {},
   "source": [
    "**Question 5: What are some common real-world applications of Decision Trees, and \n",
    "what are their main advantages and limitations?**\n",
    "\n",
    "**1.Healthcare and Medicine**\n",
    "\n",
    "- Medical Diagnosis: Diagnosing diseases based on symptoms and test results\n",
    "- Treatment Planning: Determining optimal treatment protocols\n",
    "- Drug Discovery: Identifying potential drug compounds\n",
    "- Healthcare Resource Allocation: Predicting patient admission needs\n",
    "\n",
    "Example: Emergency room triage systems use decision trees to prioritize patients based on symptoms, vital signs, and medical history.\n",
    "\n",
    "**2. Finance and Banking**\n",
    "\n",
    "- Credit Scoring: Evaluating loan default risk\n",
    "- Fraud Detection: Identifying suspicious transactions\n",
    "- Investment Decisions: Portfolio optimization strategies\n",
    "- Insurance Claims: Assessing claim legitimacy and pricing\n",
    "\n",
    "Example: Banks use decision trees to automatically approve or reject credit card applications based on income, credit history, and employment status.\n",
    "\n",
    "**3. Marketing and E-commerce**\n",
    "\n",
    "- Customer Segmentation: Grouping customers for targeted campaigns\n",
    "- Recommendation Systems: Suggesting products to users\n",
    "- Price Optimization: Dynamic pricing strategies\n",
    "- Churn Prediction: Identifying customers likely to leave\n",
    "\n",
    "Example: Online retailers use decision trees to recommend products based on browsing history, purchase patterns, and demographic information.\n",
    "\n",
    "**4. Manufacturing and Quality Control**\n",
    "\n",
    "- Defect Detection: Identifying faulty products\n",
    "- Predictive Maintenance: Scheduling equipment maintenance\n",
    "- Supply Chain Optimization: Managing inventory levels\n",
    "- Process Control: Monitoring production parameters\n",
    "\n",
    "Example: Automotive manufacturers use decision trees to detect defective parts on assembly lines based on sensor measurements.\n",
    "\n",
    "**5.Technology and IT**\n",
    "\n",
    "- Network Security: Detecting cyber attacks\n",
    "- Software Testing: Automated test case generation\n",
    "- System Monitoring: Predicting system failures\n",
    "- User Behavior Analysis: Understanding user interactions\n",
    "\n",
    "Example: Email providers use decision trees to classify emails as spam or legitimate based on sender, content, and metadata.\n",
    "\n",
    "**Main Advantages**\n",
    "\n",
    "1. High Interpretability  \n",
    "Decision trees are easy to visualize and understand by non-technical people. This makes them great for:  \n",
    "\n",
    "- Regulatory compliance (banking, healthcare)  \n",
    "- Business rule generation  \n",
    "- Explaining automated decisions to customers  \n",
    "- Training staff on decision processes  \n",
    "\n",
    "2. No Data Preprocessing Requirements  \n",
    "\n",
    "- Handles missing values and can work with incomplete data.  \n",
    "- Works with both numerical and categorical features.  \n",
    "- Doesn't require feature normalization.  \n",
    "- Tree splits are not heavily influenced by extreme values.  \n",
    "\n",
    "3. Automatic Feature Selection  \n",
    "\n",
    "- Identifies the most important features naturally.  \n",
    "- Reduces dimensionality automatically.  \n",
    "- Eliminates irrelevant features.  \n",
    "- Shows feature importance rankings.  \n",
    "\n",
    "4. Fast Prediction  \n",
    "\n",
    "- Constant time complexity for predictions O(log n).  \n",
    "- Suitable for real-time applications.  \n",
    "- Low computational requirements for deployment.  \n",
    "- Easy to implement in production systems.  \n",
    "\n",
    "5. Handles Non-Linear Relationships  \n",
    "\n",
    "- Captures complex interactions between features.  \n",
    "- Makes no assumptions about data distribution.  \n",
    "- Models non-monotonic relationships effectively.  \n",
    "- Adapts to data patterns naturally.  \n",
    "\n",
    "**Main Limitations**\n",
    "\n",
    "1. Overfitting Tendency  \n",
    "- Problem: Trees can become too complex and memorize training data instead of learning general patterns.  \n",
    "- Real-world impact: A medical diagnosis tree might create very specific rules that work perfectly on training cases but fail on new patients with slightly different symptoms.  \n",
    "- Mitigation: Use pruning techniques, cross-validation, and ensemble methods.  \n",
    "\n",
    "2. Instability  \n",
    "- Problem: Small changes in training data can lead to very different tree structures.  \n",
    "- Real-world impact: A credit scoring model might approve different loan applications if just a few training examples change, resulting in inconsistent business decisions.  \n",
    "- Mitigation: Use ensemble methods like Random Forests or bootstrap aggregating.  \n",
    "\n",
    "3. Bias Toward Features with More Levels  \n",
    "- Problem: Features with many possible values (like zip codes) are favored over binary features.  \n",
    "- Real-world impact: A customer segmentation model might focus too much on location data while ignoring important binary indicators like \"premium customer\" status.  \n",
    "- Mitigation: Use Gain Ratio instead of Information Gain or preprocess features appropriately.  \n",
    "\n",
    "4. Difficulty with Linear Relationships  \n",
    "- Problem: Trees create axis-parallel splits, making them inefficient for diagonal decision boundaries.  \n",
    "- Real-world impact: In fraud detection, a linear combination of transaction amount and frequency might be more predictive than individual thresholds, but decision trees struggle to capture this effectively.  \n",
    "- Mitigation: Feature engineering to create interaction terms or use ensemble methods.  \n",
    "\n",
    "5. Limited Continuous Value Prediction  \n",
    "- Problem: For regression tasks, trees predict step functions rather than smooth curves.  \n",
    "- Real-world impact: Stock price prediction models using decision trees produce choppy, unrealistic price forecasts instead of smooth trends.  \n",
    "- Mitigation: Use ensemble methods or consider other algorithms for smooth regression tasks.  \n",
    "\n",
    "Decision trees remain popular because they balance performance and interpretability. This makes them valuable tools for both exploratory analysis and production systems where explainability is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd819c83",
   "metadata": {},
   "source": [
    "**Question 6:   Write a Python program to:**\n",
    "- Load the Iris Dataset \n",
    "- Train a Decision Tree Classifier using the Gini criterion \n",
    "- Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641dbea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Decision Tree Classifier with Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 5. Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "\n",
    "# 6. Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec77aed",
   "metadata": {},
   "source": [
    "**Question 7:  Write a Python program to:**\n",
    "- Load the Iris Dataset \n",
    "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to \n",
    "a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a9f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Comparison Results:\n",
      "Limited Tree (max_depth=3) Accuracy: 1.0000\n",
      "Fully-grown Tree Accuracy: 1.0000\n",
      "Difference: 0.0000\n",
      "\n",
      "Tree Structure:\n",
      "Limited Tree - Depth: 3, Leaves: 5\n",
      "Full Tree - Depth: 6, Leaves: 10\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree with max_depth=3\n",
    "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_limited.fit(X_train, y_train)\n",
    "pred_limited = tree_limited.predict(X_test)\n",
    "accuracy_limited = accuracy_score(y_test, pred_limited)\n",
    "\n",
    "# Train fully-grown Decision Tree\n",
    "tree_full = DecisionTreeClassifier(random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "pred_full = tree_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, pred_full)\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"Decision Tree Comparison Results:\")\n",
    "print(f\"Limited Tree (max_depth=3) Accuracy: {accuracy_limited:.4f}\")\n",
    "print(f\"Fully-grown Tree Accuracy: {accuracy_full:.4f}\")\n",
    "print(f\"Difference: {abs(accuracy_full - accuracy_limited):.4f}\")\n",
    "\n",
    "# Tree structure comparison\n",
    "print(f\"\\nTree Structure:\")\n",
    "print(f\"Limited Tree - Depth: {tree_limited.get_depth()}, Leaves: {tree_limited.get_n_leaves()}\")\n",
    "print(f\"Full Tree - Depth: {tree_full.get_depth()}, Leaves: {tree_full.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b841769",
   "metadata": {},
   "source": [
    "**Question 8: Write a Python program to:**\n",
    "- Load the California Housing dataset from sklearn \n",
    "- Train a Decision Tree Regressor \n",
    "- Print the Mean Squared Error (MSE) and feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a9a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor Results:\n",
      "Mean Squared Error (MSE): 0.4952\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5285\n",
      "HouseAge: 0.0519\n",
      "AveRooms: 0.0530\n",
      "AveBedrms: 0.0287\n",
      "Population: 0.0305\n",
      "AveOccup: 0.1308\n",
      "Latitude: 0.0937\n",
      "Longitude: 0.0829\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Decision Tree Regressor Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i, importance in enumerate(regressor.feature_importances_):\n",
    "    print(f\"{housing.feature_names[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894e664",
   "metadata": {},
   "source": [
    "**Question 9: Write a Python program to:** \n",
    "- Load the Iris Dataset \n",
    "- Tune the Decision Tree’s max_depth and min_samples_split using \n",
    "GridSearchCV \n",
    "- Print the best parameters and the resulting model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1cc796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV Results:\n",
      "Best Parameters: {'max_depth': 7, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.9417\n",
      "Test Accuracy with Best Model: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Create Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Test the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"GridSearchCV Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy with Best Model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f02bae",
   "metadata": {},
   "source": [
    "**Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
    "Explain the step-by-step process you would follow to: \n",
    "- Handle the missing values \n",
    "- Encode the categorical features \n",
    "- Train a Decision Tree model \n",
    "- Tune its hyperparameters \n",
    "- Evaluate its performance \n",
    "And describe what business value this model could provide in the real-world setting.\n",
    "\n",
    "**Predicting Disease with Decision Trees**\n",
    "\n",
    "### 1. Handling Missing Values\n",
    "\n",
    "Missing data can affect results, so it’s important to address it. I would use a **Strategic Imputation** approach:\n",
    "\n",
    "* **Numerical Features:** For continuous data like age or blood pressure, I'd fill in missing values with the **median**. The median is less affected by outliers than the mean, making it a reliable choice.\n",
    "* **Categorical Features:** For features like blood type or gender, I'd use the **mode** (the most common category) to fill in missing values. Alternatively, I could create a new category called \"Missing\" to indicate that the data was not available. This can sometimes serve as a predictive signal.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Encoding Categorical Features\n",
    "\n",
    "Machine learning models need numerical input. I would convert categorical data into a numerical format using **One-Hot Encoding**.\n",
    "\n",
    "* **Process:** This technique creates a new binary column for each unique category. For instance, a \"Blood Type\" column with values \"A\", \"B\", and \"O\" would turn into three new columns: \"Blood Type_A\", \"Blood Type_B\", and \"Blood Type_O\". A value of 1 indicates the presence of that category, and 0 indicates its absence. This avoids the model assuming any order or hierarchy between categories.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Training a Decision Tree Model\n",
    "\n",
    "After preprocessing, I would train a Decision Tree model on the prepared data.\n",
    "\n",
    "* **Splitting the Data:** I'd first divide the dataset into a **training set** (e.g., 80%) to train the model and a **testing set** (e.g., 20%) to check its performance on unseen data.\n",
    "* **Training:** I would then fit the Decision Tree classifier to the training data. The algorithm learns a set of rules from the features to predict the target variable (whether the disease is present or not).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Tuning Hyperparameters\n",
    "\n",
    "A default Decision Tree can easily overfit the training data. I would adjust its hyperparameters to improve its ability to handle new data.\n",
    "\n",
    "* **Key Hyperparameters:**\n",
    "    * `max_depth`: The maximum depth of the tree. Limiting this keeps the model from becoming too complex and memorizing the training data.\n",
    "    * `min_samples_leaf`: The minimum number of samples needed to be at a leaf node. This ensures that each leaf represents a meaningful number of data points.\n",
    "* **Method:** I would use **Grid Search with Cross-Validation**. This method tests all possible combinations of a set range of hyperparameter values, using part of the training data (k-folds) for validation. The combination that yields the best performance is chosen as the optimal set of hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evaluating Performance\n",
    "\n",
    "Evaluating the model is essential to ensure it’s reliable for medical use.\n",
    "\n",
    "* **Confusion Matrix:** I would use a confusion matrix to compare the model's predictions with actual outcomes. This will show True Positives, True Negatives, False Positives, and False Negatives.\n",
    "* **Metrics:**\n",
    "    * **Recall (Sensitivity):** This is the most important metric. It measures the proportion of actual disease cases that the model correctly identifies. In healthcare, a high recall is crucial to avoid missing positive cases (False Negatives), which could have serious consequences. The formula for recall is: $Recall = TP / (TP + FN)$.\n",
    "    * **Precision:** This measures the proportion of positive predictions that were actually correct. The formula for precision is: $Precision = TP / (TP + FP)$.\n",
    "    * **F1-Score:** The harmonic mean of precision and recall. It provides a balanced measure of the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Business Value\n",
    "\n",
    "This predictive model offers significant business value in a healthcare environment:\n",
    "\n",
    "* **Early Diagnosis:** The model could identify patients with a high likelihood of having a disease, prompting doctors to order confirmatory tests sooner. This enables earlier intervention and treatment, leading to better patient outcomes and potentially lower long-term healthcare costs.\n",
    "* **Resource Optimization:** By pinpointing high-risk patients, hospitals can allocate resources more effectively, prioritizing diagnostic tests and specialist consultations. This can reduce unnecessary spending on low-risk patients.\n",
    "* **Improved Patient Management:** The model can assist in risk stratification, allowing healthcare providers to tailor care plans and preventive measures for patients who are most likely to benefit.\n",
    "* **Enhanced Research & Development:** Insights from the model can help researchers identify which features (e.g., specific symptoms, lab results) are the most important predictors of the disease. This can speed up the development of new diagnostic tools and treatments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
