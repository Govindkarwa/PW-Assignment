{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd81e9b0",
   "metadata": {},
   "source": [
    "#### **Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm generally used for classification but can also be used for regression tasks. It works by finding the \"k\" closest data points (neighbors) to a given input and makes a predictions based on the majority class (for classification) or the average value (for regression). Since KNN makes no assumptions about the underlying data distribution it makes it a non-parametric and instance-based learning method.\n",
    "K-Nearest Neighbors is also called as a lazy learner algorithm because it does not learn from the training set immediately instead it stores the entire dataset and performs computations only at the time of classification.\n",
    "\n",
    "**Working of KNN algorithm**\n",
    "\n",
    "Thе K-Nearst Neighbors (KNN) algorithm operates on the principle of similarity where it predicts the label or value of a new data point by considering the labels or values of its K nearest neighbors in the training dataset.\n",
    "\n",
    "**Step 1: Selecting the optimal value of K**\n",
    "- K represents the number of nearest neighbors that needs to be considered while making prediction.\n",
    "\n",
    "**Step 2: Calculating distance**\n",
    "- To measure the similarity between target and training data points Euclidean distance is widely used. Distance is calculated between data points in the dataset and target point.\n",
    "\n",
    "**Step 3: Finding Nearest Neighbors**\n",
    "- The k data points with the smallest distances to the target point are nearest neighbors.\n",
    "\n",
    "**Step 4: Voting for Classification or Taking Average for Regression(Make prediction)**\n",
    "\n",
    "- When you want to classify a data point into a category like spam or not spam, the KNN algorithm looks at the K closest points in the dataset. These closest points are called neighbors. The algorithm then looks at which category the neighbors belong to and picks the one that appears the most. This is called majority voting.\n",
    "\n",
    "- In regression, the algorithm still looks for the K closest points. But instead of voting for a class in classification, it takes the average of the values of those K neighbors. This average is the predicted value for the new point for the algorithm.\n",
    "\n",
    "**KNN for Classification**\n",
    "\n",
    "- Suppose we want to predict whether a fruit is apple or orange.\n",
    "\n",
    "- We look at the K nearest fruits.\n",
    "\n",
    "- If most neighbors are apples, we classify the new fruit as an apple.\n",
    "\n",
    "- Example: If K=5 and among the 5 neighbors → 3 are apples, 2 are oranges → prediction = apple.\n",
    "\n",
    "**KNN for Regression**\n",
    "\n",
    "- Suppose we want to predict the price of a house.\n",
    "\n",
    "- We look at the K nearest houses (based on features like size, location, etc.).\n",
    "\n",
    "- The prediction is usually the average price of those K houses.\n",
    "\n",
    "- Example: If K=3 and house prices of neighbors = [50L, 55L, 60L] → prediction = (50+55+60)/3 = 55L.\n",
    "\n",
    "**Advantages of KNN**\n",
    "- Simple to use: Easy to understand and implement.\n",
    "- No training step: No need to train as it just stores the data and uses it during prediction.\n",
    "- Few parameters: Only needs to set the number of neighbors (k) and a distance method.\n",
    "- Versatile: Works for both classification and regression problems.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### **Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
    "\n",
    "**What is the Curse of Dimensionality?**\n",
    "- The Curse of Dimensionality refers to various phenomena that arise when dealing with high-dimensional data.\n",
    "- As the number of features or dimensions increases, the volume of the feature space grows exponentially, leading to sparsity in the data distribution.\n",
    "- This sparsity can result in several challenges such as increased computational complexity, overfitting, and deteriorating performance of certain algorithms.\n",
    "\n",
    "**How does Dimensionality effect KNN Performance?**\n",
    "\n",
    "The impact of dimensionality on the performance of KNN (K-Nearest Neighbors) is a well-known issue in machine learning. Here's a breakdown of how dimensionality affects KNN performance:\n",
    "\n",
    "- **Increased Sparsity:** As the number of dimensions increases, the volume of the space grows exponentially. Consequently, the available data becomes sparser, meaning that data points are spread farther apart from each other. This sparsity can lead to difficulties in finding meaningful nearest neighbors, as there may be fewer neighboring points within a given distance.\n",
    "\n",
    "- **Equal Distances:** In high-dimensional spaces, the concept of distance becomes less meaningful. As the number of dimensions increases, the distance between any two points tends to become more uniform, or equidistant. This phenomenon occurs because the influence of any single dimension diminishes as the number of dimensions grows, leading to points being distributed more uniformly across the space.\n",
    "\n",
    "- **Degraded Performance:** KNN relies on the assumption that nearby points in the feature space are likely to have similar labels. However, in high-dimensional spaces, this assumption may no longer hold true due to the increased sparsity and equalization of distances. As a result, KNN may struggle to accurately classify data points, leading to degraded performance.\n",
    "\n",
    "- **Increased Computational Complexity:** With higher dimensionality, the computational cost of KNN increases significantly. The algorithm needs to compute distances in a high-dimensional space, which involves more calculations. This can make the KNN algorithm slower and less efficient, especially when dealing with large datasets.\n",
    "\n",
    "The curse of dimensionality makes distances meaningless in high dimensions, causing KNN to perform poorly. That’s why KNN works best in low to moderate-dimensional data after proper feature selection or dimensionality reduction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730b1eb",
   "metadata": {},
   "source": [
    "**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in data analysis and machine learning. It helps you to reduce the number of features in a dataset while keeping the most important information. It changes your original features into new features these new features don’t overlap with each other and the first few keep most of the important differences found in the original data.\n",
    "\n",
    "In short Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by finding the directions (principal components) that capture the maximum variance in the data.\n",
    "\n",
    "**How Principal Component Analysis Works**\n",
    "\n",
    "PCA uses linear algebra to transform data into new features called principal components. It finds these by calculating eigenvectors (directions) and eigenvalues (importance) from the covariance matrix. PCA selects the top components with the highest eigenvalues and projects the data onto them simplify the dataset.\n",
    "\n",
    "**PCA vs Feature Selection**\n",
    "\n",
    "Principal Component Analysis (PCA) and feature selection are both dimensionality reduction techniques but they work differently. PCA is a feature extraction method that transforms the original correlated features into a new set of uncorrelated features called principal components, which are linear combinations of the original features and are ordered by how much variance they capture. This makes PCA powerful for removing redundancy, but the new features are harder to interpret. In contrast, feature selection is a feature elimination method that simply selects the most relevant original features and discards the less useful or redundant ones. Unlike PCA, feature selection keeps the features in their original form, which makes them easier to understand and directly usable in analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae0074",
   "metadata": {},
   "source": [
    "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
    "\n",
    "In Principal Component Analysis (PCA), eigenvalues are scalar values that represent the amount of variance captured by each principal component. They are derived from the covariance matrix of the dataset and are used to determine the significance of each component.\n",
    "\n",
    "Eigenvectors are the vectors indicating the direction of the axes along which the data varies the most. Each eigenvector has a corresponding eigenvalue, quantifying the amount of variance captured along its direction.\n",
    "\n",
    "\n",
    "**Why They're Important**\n",
    "\n",
    "Eigenvalues and eigenvectors are crucial for dimensionality reduction and data visualization in PCA.\n",
    "\n",
    "- **Eigenvectors Determine Direction:** The eigenvectors define the directions of maximum variance in the data. The eigenvector with the highest eigenvalue is the first principal component, representing the most significant dimension of the data.\n",
    "\n",
    "- **Eigenvalues Determine Importance:** The magnitude of an eigenvalue indicates the amount of variance captured by its corresponding eigenvector. Larger eigenvalues mean more information is captured, while smaller ones represent noise or less significant data dimensions. This allows you to select only the most important components (those with the largest eigenvalues) and discard the rest, effectively reducing the dimensionality of the dataset without losing much of the important information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c1bf8",
   "metadata": {},
   "source": [
    "#### **Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
    "\n",
    "Principal Component Analysis (PCA) and K-Nearest Neighbors (KNN) are often used together in a machine learning pipeline to improve model performance. PCA is a dimensionality reduction technique, and KNN is a classification or regression algorithm. When combined, PCA is typically used as a preprocessing step before applying KNN.This combination is a classic example of how a data transformation step can solve key problems for a downstream model. PCA complements KNN by addressing its major weaknesses, which are its sensitivity to the curse of dimensionality and its computational inefficiency with high-dimensional data.\n",
    "\n",
    "**KNN and PCA complement each other very well when applied in a single pipeline**\n",
    "\n",
    "- KNN relies on distance between data points for classification or regression. However, in high-dimensional datasets, distances become less meaningful (curse of dimensionality), noisy features disturb distance calculations, and computation becomes heavy.\n",
    "\n",
    "- PCA reduces dimensionality by projecting data onto a smaller set of principal components that capture the most important variance. This removes redundant/noisy features, makes distance measures more reliable, and speeds up computations.\n",
    "\n",
    "- Together, PCA improves KNN’s accuracy (by focusing on informative features), efficiency (by reducing feature space), and interpretability (by allowing visualization of data in 2D/3D), while KNN provides a simple yet powerful classifier or regressor on the transformed space.\n",
    "\n",
    "In short: PCA prepares the data by cleaning and compressing it, and KNN benefits by making better, faster, and more interpretable predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5eddd5",
   "metadata": {},
   "source": [
    "#### **Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41229c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.7407\n",
      "Accuracy with scaling: 0.9630\n",
      "Improvement: 22.22%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN WITHOUT scaling\n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scale.fit(X_train, y_train)\n",
    "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
    "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    "\n",
    "# KNN WITH scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_with_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_with_scale.fit(X_train_scaled, y_train)\n",
    "y_pred_with_scale = knn_with_scale.predict(X_test_scaled)\n",
    "accuracy_with_scale = accuracy_score(y_test, y_pred_with_scale)\n",
    "\n",
    "# Results\n",
    "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
    "print(f\"Accuracy with scaling: {accuracy_with_scale:.4f}\")\n",
    "print(f\"Improvement: {(accuracy_with_scale - accuracy_no_scale)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e63e8",
   "metadata": {},
   "source": [
    "#### **Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc706568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio of each principal component:\n",
      "PC1: 0.3620 (36.20%)\n",
      "PC2: 0.1921 (19.21%)\n",
      "PC3: 0.1112 (11.12%)\n",
      "PC4: 0.0707 (7.07%)\n",
      "PC5: 0.0656 (6.56%)\n",
      "PC6: 0.0494 (4.94%)\n",
      "PC7: 0.0424 (4.24%)\n",
      "PC8: 0.0268 (2.68%)\n",
      "PC9: 0.0222 (2.22%)\n",
      "PC10: 0.0193 (1.93%)\n",
      "PC11: 0.0174 (1.74%)\n",
      "PC12: 0.0130 (1.30%)\n",
      "PC13: 0.0080 (0.80%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# 2. Standardize features (important before PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Apply PCA (keep all components)\n",
    "pca = PCA(n_components=X.shape[1])   # same number of components as features\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 4. Print explained variance ratio\n",
    "print(\"Explained variance ratio of each principal component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69623d",
   "metadata": {},
   "source": [
    "#### **Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bca4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Results:\n",
      "Original dataset (13 features): 0.9630 (96.30%)\n",
      "PCA dataset (2 components):     0.9815 (98.15%)\n",
      "Accuracy difference:            -0.0185 (-1.85%)\n",
      "\n",
      "Variance explained by top 2 PCs: 0.5496 (54.96%)\n",
      "✓ PCA maintained/improved accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KNN on ORIGINAL scaled dataset (all 13 features)\n",
    "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = knn_original.predict(X_test_scaled)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# Apply PCA (retain top 2 components)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# KNN on PCA-transformed dataset (2 components)\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# Results\n",
    "print(\"KNN Classification Results:\")\n",
    "print(f\"Original dataset (13 features): {accuracy_original:.4f} ({accuracy_original*100:.2f}%)\")\n",
    "print(f\"PCA dataset (2 components):     {accuracy_pca:.4f} ({accuracy_pca*100:.2f}%)\")\n",
    "print(f\"Accuracy difference:            {accuracy_original - accuracy_pca:.4f} ({(accuracy_original - accuracy_pca)*100:.2f}%)\")\n",
    "\n",
    "# Show variance explained by top 2 components\n",
    "variance_explained = sum(pca.explained_variance_ratio_)\n",
    "print(f\"\\nVariance explained by top 2 PCs: {variance_explained:.4f} ({variance_explained*100:.2f}%)\")\n",
    "\n",
    "if accuracy_pca < accuracy_original:\n",
    "   print(\"✗ PCA reduced accuracy (information loss)\")\n",
    "else:\n",
    "   print(\"✓ PCA maintained/improved accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773113ea",
   "metadata": {},
   "source": [
    "#### **Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634e5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Different Distance Metrics:\n",
      "Euclidean distance: 0.9444 (94.44%)\n",
      "Manhattan distance: 0.9815 (98.15%)\n",
      "Accuracy difference: -0.0370 (-3.70%)\n",
      "✓ Manhattan distance performs better\n",
      "\n",
      "Distance Metric Explanation:\n",
      "• Euclidean: √(Σ(xi - yi)²) - straight-line distance\n",
      "• Manhattan: Σ|xi - yi| - city block distance\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KNN with Euclidean distance (default)\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean',p=2)\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# KNN with Manhattan distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan',p=1)\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# Results\n",
    "print(\"KNN with Different Distance Metrics:\")\n",
    "print(f\"Euclidean distance: {accuracy_euclidean:.4f} ({accuracy_euclidean*100:.2f}%)\")\n",
    "print(f\"Manhattan distance: {accuracy_manhattan:.4f} ({accuracy_manhattan*100:.2f}%)\")\n",
    "\n",
    "difference = accuracy_euclidean - accuracy_manhattan\n",
    "print(f\"Accuracy difference: {difference:.4f} ({difference*100:.2f}%)\")\n",
    "\n",
    "if accuracy_euclidean > accuracy_manhattan:\n",
    "   print(\"✓ Euclidean distance performs better\")\n",
    "elif accuracy_manhattan > accuracy_euclidean:\n",
    "   print(\"✓ Manhattan distance performs better\")\n",
    "else:\n",
    "   print(\"= Both distances perform equally\")\n",
    "\n",
    "print(\"\\nDistance Metric Explanation:\")\n",
    "print(\"• Euclidean: √(Σ(xi - yi)²) - straight-line distance\")\n",
    "print(\"• Manhattan: Σ|xi - yi| - city block distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1dc73",
   "metadata": {},
   "source": [
    "#### **Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.** \n",
    "**Due to the large number of features and a small number of samples, traditional models overfit.** \n",
    "\n",
    "Explain how you would: \n",
    "- Use PCA to reduce dimensionality \n",
    "- Decide how many components to keep \n",
    "- Use KNN for classification post-dimensionality reduction \n",
    "- Evaluate the model \n",
    "- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data \n",
    "\n",
    "\n",
    "## **Using PCA and KNN for Cancer Classification**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. PCA for Dimensionality Reduction**\n",
    "\n",
    "* Gene expression datasets usually contain **thousands of features (genes)** but only a **small number of patients (samples)**.\n",
    "* Training a model directly on this raw dataset would cause **overfitting**, because the model could memorize noise instead of learning real patterns.\n",
    "* **Principal Component Analysis (PCA)** solves this by transforming the original features into a smaller set of **principal components**.\n",
    "* These components are **linear combinations of the original genes** and are ordered by how much variation (information) they capture.\n",
    "* By projecting the dataset into this lower-dimensional space, we keep the most meaningful structure while discarding noise and redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Deciding How Many Components to Keep**\n",
    "\n",
    "* PCA produces as many components as original features, but not all are equally useful.\n",
    "* To decide how many to keep, we look at the **explained variance ratio**: how much of the dataset’s variability is captured by each component.\n",
    "* We calculate the **cumulative explained variance curve**, which shows how variance adds up as more components are included.\n",
    "* A common rule is to keep enough components to explain **90–95% of the variance**, which balances **information retention** and **dimensionality reduction**.\n",
    "* Example: If 20 components out of 500 capture 95% of the variance, we can reduce from 500 genes to 20 features without losing much signal.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. KNN for Classification**\n",
    "\n",
    "* After dimensionality reduction, we train a **K-Nearest Neighbors (KNN)** classifier on the new dataset.\n",
    "* KNN works by finding the **closest patients (neighbors)** in the reduced feature space and assigning a cancer type based on the **majority vote**.\n",
    "* This approach makes sense here because PCA has made distances more reliable by removing noisy or irrelevant features.\n",
    "* We use **cross-validation** to tune the value of `k` (the number of neighbors). Too small `k` → sensitive to noise; too large `k` → overly smooth predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Evaluating the Model**\n",
    "\n",
    "* To measure performance, we hold out a portion of the data as a **test set** that the model never sees during training.\n",
    "* We calculate metrics:\n",
    "\n",
    "  * **Accuracy**: overall correct predictions.\n",
    "  * **Precision**: how many predicted patients of a certain cancer type are truly that type.\n",
    "  * **Recall (Sensitivity)**: how many patients of a given cancer type were correctly identified.\n",
    "  * **F1-score**: balance between precision and recall, especially useful when cancer types are imbalanced.\n",
    "* These metrics together ensure we aren’t just doing well on the most common cancer but are reliable across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Justification to Stakeholders**\n",
    "\n",
    "* **Challenge addressed**: Biomedical datasets usually have far more genes than patients, which leads to overfitting. PCA reduces dimensionality, helping the model generalize better.\n",
    "* **Biological signal retention**: By keeping the top principal components, we focus on the strongest patterns in the gene expression data, which often correspond to real biological differences between cancer types.\n",
    "* **Efficiency**: Fewer features mean faster computation and easier storage, which is practical in real-world pipelines.\n",
    "* **Transparency**: KNN is a simple, non-parametric model. Its predictions can be explained directly (e.g., “this patient is classified based on their similarity to these neighbors”).\n",
    "* **Robustness**: PCA + KNN together make a defensible, efficient, and interpretable solution that works well on **small-sample, high-dimensional biomedical data**—a very common scenario in cancer genomics research.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
