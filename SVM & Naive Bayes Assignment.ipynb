{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0e153a",
   "metadata": {},
   "source": [
    "**Question 1:  What is a Support Vector Machine (SVM), and how does it work?**\n",
    "\n",
    " Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It works by finding the optimal boundary (called a hyperplane) that best separates different classes of data.\n",
    "\n",
    "**Core Concept**\n",
    "\n",
    "The fundamental idea behind SVM is to find the hyperplane that maximizes the margin between different classes. The margin is the distance between the hyperplane and the nearest data points from each class. These nearest points are called \"support vectors\" because they literally support or define the decision boundary.\n",
    "\n",
    "**How SVM Works**\n",
    "\n",
    "The main idea of SVM is to find the best decision boundary (called a hyperplane) that separates data points of different classes with the maximum margin.\n",
    "\n",
    "1.Separating Hyperplane\n",
    "\n",
    "- For a binary classification problem, SVM tries to find a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions) that divides the data into two classes.\n",
    "\n",
    "2.Margin Maximization\n",
    "\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class.\n",
    "\n",
    "- SVM chooses the hyperplane that maximizes this margin, making the model more robust to new data.\n",
    "\n",
    "3.Support Vectors\n",
    "\n",
    "- The data points that lie closest to the hyperplane are called support vectors.\n",
    "\n",
    "- These points are critical because they determine the position and orientation of the hyperplane.\n",
    "\n",
    "4.Handling Non-linear Data (Kernel Trick)\n",
    "\n",
    "- If the data is not linearly separable, SVM uses a kernel function to transform the data into a higher-dimensional space where a linear separator can be found.\n",
    "\n",
    "- Common kernels:\n",
    "\n",
    "- Linear Kernel → works for linearly separable data.\n",
    "\n",
    "- Polynomial Kernel → handles curved boundaries.\n",
    "\n",
    "- RBF (Radial Basis Function / Gaussian) Kernel → popular for complex, non-linear data.\n",
    "\n",
    "**Key Advantages**\n",
    "SVM is particularly effective because it focuses on the most informative data points (support vectors) rather than all training data. It's also robust to overfitting, especially in high-dimensional spaces, and works well even with limited training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b55df7",
   "metadata": {},
   "source": [
    "**Question 2: Explain the difference between Hard Margin and Soft Margin SVM.**\n",
    "\n",
    "The distinction between Hard Margin and Soft Margin SVM relates to how strictly the algorithm enforces the separation between classes and handles data that may not be perfectly separable.\n",
    "\n",
    "### Hard Margin SVM \n",
    "\n",
    "Definition: In hard margin SVM, the goal is to find a hyperplane that perfectly separates the data points of different classes without any misclassifications. This is only feasible when the data is linearly separable.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- All training points must be correctly classified\n",
    "- No data points are allowed to fall within the margin or on the wrong side of the decision boundary\n",
    "- The optimization problem seeks to maximize the margin without any exceptions\n",
    "- Results in a rigid decision boundary\n",
    "\n",
    "**Mathematical formulation:**\n",
    "The constraint is strict: yi(w·xi + b) ≥ 1 for all training points, where yi is the class label and w·xi + b defines the hyperplane.\n",
    "\n",
    "**Advantages of Hard Margin**\n",
    "- Guaranteed Separation: Hard margin SVM ensures that the classes are perfectly separated, leading to optimal generalization performance when the training data is linearly separable.\n",
    "- Simplicity: The optimization problem in hard margin SVM is well-defined and has a unique solution, making it computationally efficient.\n",
    "\n",
    "**Disadvantages of Hard Margin**\n",
    "- Sensitivity to Outliers: Hard margin SVM is highly sensitive to outliers or noisy data points. Even a single mislabeled point can significantly affect the position of the decision boundary and lead to poor generalization on unseen data.\n",
    "- Not Suitable for Non-linear Data: When the data is not linearly separable, hard margin SVM fails to find a valid solution, rendering it impractical for many real-world datasets.\n",
    "\n",
    "### Soft Margin SVM\n",
    "\n",
    "Definition: Soft margin SVM introduces flexibility by allowing some misclassifications. This approach is useful when the data is not perfectly separable or when there are outliers.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Permits some training points to be misclassified or fall within the margin\n",
    "- Introduces \"slack variables\" (ξi) that measure how much each point violates the ideal margin\n",
    "- Balances between maximizing the margin and minimizing classification errors\n",
    "- More robust to outliers and noise\n",
    "\n",
    "**Mathematical formulation:**\n",
    "The constraint becomes: yi(w·xi + b) ≥ 1 - ξi, where ξi ≥ 0 are the slack variables.\n",
    "\n",
    "\n",
    "**Advantages of Soft Margin SVM**\n",
    "- Robustness to Outliers: Soft margin SVM can handle outliers or noisy data more effectively by allowing for some misclassifications. This results in a more robust decision boundary that generalizes better to unseen data.\n",
    "- Applicability to Non-linear Data: Unlike hard margin SVM, soft margin SVM can handle non-linearly separable data by implicitly mapping it to a higher-dimensional space using kernel functions. This enables SVM to capture complex decision boundaries.\n",
    "\n",
    "**Disadvantages of Soft Margin SVM**\n",
    "- Need for Parameter Tuning: The performance of soft margin SVM heavily depends on the choice of the regularization parameter C. Selecting an appropriate value for C requires careful tuning, which can be time-consuming and computationally expensive, especially for large datasets.\n",
    "- Potential Overfitting: In cases where the value of C is too large, soft margin SVM may overfit the training data by allowing too many margin violations.\n",
    "\n",
    "**In short:**\n",
    "\n",
    "Hard Margin SVM → Strict, no errors, only works with clean linearly separable data.\n",
    "\n",
    "Soft Margin SVM → Flexible, allows some misclassifications, better for real-world noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77c926",
   "metadata": {},
   "source": [
    "**Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.**\n",
    "\n",
    "**Kernel Trick in SVM**\n",
    "\n",
    "The Kernel Trick is a mathematical technique that allows SVM to classify data that is not linearly separable by implicitly mapping it into a higher-dimensional feature space without actually computing the transformation.\n",
    "\n",
    "- Instead of explicitly transforming features, we use a kernel function that computes the inner product in the higher-dimensional space directly.\n",
    "\n",
    "- This saves computation and makes SVM efficient even in very high dimensions.\n",
    "\n",
    "Example: Radial Basis Function (RBF) Kernel\n",
    "Formula:\n",
    "K(xi, xj) = exp(-γ||xi - xj||²)\n",
    "Where:\n",
    "\n",
    "- ||xi - xj||² is the squared Euclidean distance between points\n",
    "- γ (gamma) is a parameter that controls the kernel's width\n",
    "\n",
    "How RBF Works:\n",
    "\n",
    "- When two points are close (small distance), K approaches 1\n",
    "- When two points are far apart (large distance), K approaches 0\n",
    "- This creates \"influence zones\" around each support vector\n",
    "\n",
    "\n",
    "**RBF Use Case: XOR Problem** \n",
    "\n",
    "Consider the classic XOR dataset:\n",
    "- Points (0,0) and (1,1) belong to Class A\n",
    "- Points (0,1) and (1,0) belong to Class B\n",
    "\n",
    "This data is not linearly separable in 2D space - no straight line can separate these classes.\n",
    "\n",
    "**With RBF Kernel:**\n",
    "The RBF kernel creates circular decision boundaries around support vectors. Each support vector acts like a \"radial influence center,\" and the final decision boundary becomes a combination of these circular regions.\n",
    "\n",
    "**Parameter Tuning:**\n",
    "- Small γ (wide kernel): Creates smooth, large influence zones - may underfit\n",
    "- Large γ (narrow kernel): Creates tight, precise boundaries around points - may overfit\n",
    "\n",
    "**Why RBF is Popular:**\n",
    "\n",
    "- Flexibility: Can model complex, non-linear relationships\n",
    "- Universal approximator: Can theoretically approximate any continuous function\n",
    "- Good default choice: Works well across many different types of problems\n",
    "- Handles local patterns: Effective when decision boundaries depend on local neighborhoods\n",
    "\n",
    "**Practical Applications:**\n",
    "\n",
    "- Image classification (pixel relationships are often non-linear)\n",
    "- Text mining (document similarity based on feature overlap)\n",
    "- Bioinformatics (gene expression patterns)\n",
    "- Financial modeling (market behavior prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339865",
   "metadata": {},
   "source": [
    "**Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?**\n",
    "\n",
    "**Naïve Bayes Classifier**\n",
    "Naïve Bayes is a probabilistic machine learning algorithm used for classification tasks. It predicts the class of a data point by calculating the probability that it belongs to each possible class, then assigns it to the class with the highest probability.\n",
    "\n",
    "- Naïve Bayes is a probabilistic machine learning classifier based on Bayes’ Theorem.\n",
    "- It is mainly used for classification tasks (spam filtering, sentiment analysis, text classification, medical diagnosis, etc.).\n",
    "- It assumes that the features are independent given the class label (this is the \"naïve\" assumption).\n",
    "\n",
    "**Why Is It Called \"Naïve\"**\n",
    "\n",
    "The Naïve Bayes Classifier is called “naïve” because it makes a simplifying assumption:it assumes that all features are independent of each other given the class label.\n",
    "\n",
    "Example:\n",
    "In email spam detection with features like:\n",
    "\n",
    "- Contains word \"free\"\n",
    "- Contains word \"money\"\n",
    "- Has many exclamation marks\n",
    "\n",
    "Naïve Bayes assumes these features are independent - that knowing an email contains \"free\" tells you nothing about whether it also contains \"money\". In reality, spam emails often contain both words together, so the features are actually dependent.\n",
    "\n",
    "- In reality, this assumption is almost never true since features often have correlations (for example, in text classification, words like “good” and “excellent” often appear together).\n",
    "\n",
    "- However, this assumption makes the computation much simpler, because instead of calculating a complex joint probability distribution, the model can just multiply individual probabilities.\n",
    "\n",
    "Despite being unrealistic, this “naïve” simplification works surprisingly well in practice, especially for high-dimensional problems like text classification, spam filtering, and sentiment analysis. It is also fast, efficient, and performs well even with small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67500fe9",
   "metadata": {},
   "source": [
    "**Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?**\n",
    "\n",
    "Three Main Naïve Bayes Variants\n",
    "\n",
    "**1. Gaussian Naïve Bayes**\n",
    "Gaussian Naive Bayes is a type of Naive Bayes method working on continuous attributes and the data features that follows Gaussian distribution throughout the dataset. This “naive” assumption simplifies calculations and makes the model fast and efficient. Gaussian Naive Bayes is widely used because it performs well even with small datasets and is easy to implement and interpret.\n",
    "\n",
    "**What it assumes:** Features follow a normal (Gaussian) distribution within each class.\n",
    "**How it works:** Uses mean and standard deviation to model each feature's probability distribution for each class.\n",
    "\n",
    "**When to use:**\n",
    "- Height, weight, temperature measurements\n",
    "- Sensor readings\n",
    "- Financial data (stock prices, income)\n",
    "- Any continuous variables that roughly follow a bell curve\n",
    "\n",
    "**Example:** Classifying flowers based on petal length and width measurements.\n",
    "\n",
    "**2. Multinomial Naïve Bayes**\n",
    "Multinomial Naive Bayes is one of the variation of Naive Bayes algorithm. A classification algorithm based on Bayes' Theorem ideal for discrete data and is typically used in text classification problems. It models the frequency of words as counts and assumes each feature or word is multinomially distributed. MNB is widely used for tasks like classifying documents based on word frequencies like in spam email detection.\n",
    "\n",
    "**What it assumes:** Features represent counts or frequencies that follow a multinomial distribution.\n",
    "**How it works:** Models the probability of each feature count occurring within each class.\n",
    "\n",
    "**When to use:**\n",
    "- Text classification with word counts\n",
    "- Document categorization\n",
    "- Bag-of-words models\n",
    "- Any scenario involving frequency counts\n",
    "\n",
    "**Example:** Email spam detection using word frequency counts (\"free\" appears 3 times, \"money\" appears 2 times, etc.).\n",
    "\n",
    "**3. Bernoulli Naïve Bayes**\n",
    "\n",
    "Bernoulli Naive Bayes is a subcategory of the Naive Bayes Algorithm. It is typically used when the data is binary and it models the occurrence of features using Bernoulli distribution. It is used for the classification of binary features such as 'Yes' or 'No', '1' or '0', 'True' or 'False' etc. Here it is to be noted that the features are independent of one another. In this article we will be discussing more about it.\n",
    "\n",
    "**What it assumes:** Features are binary (0 or 1) and follow a Bernoulli distribution.\n",
    "**How it works:** Models the probability of each feature being present (1) or absent (0) for each class.\n",
    "\n",
    "\n",
    "**When to use:**\n",
    "- Text classification with word presence/absence (not counts)\n",
    "- Yes/no survey responses\n",
    "- Feature presence indicators\n",
    "- Binary attributes\n",
    "\n",
    "**Example:** Document classification where you only care if specific words are present or not, regardless of how many times they appear.\n",
    "\n",
    "**Quick Selection Guide**\n",
    "- Continuous numerical data → Gaussian\n",
    "- Word/feature counts → Multinomial\n",
    "- Binary presence/absence → Bernoulli\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aaa7a5",
   "metadata": {},
   "source": [
    "**Question 6:   Write a Python program to:**\n",
    "- Load the Iris dataset \n",
    "- Train an SVM Classifier with a linear kernel \n",
    "- Print the model's accuracy and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1afd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier with Linear Kernel\n",
      "Accuracy: 1.0\n",
      "Number of Support Vectors for each class: [ 3 11 11]\n",
      "Support Vectors:\n",
      " [[4.8 3.4 1.9 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.9 2.5 4.5 1.7]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an SVM classifier with linear kernel\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"SVM Classifier with Linear Kernel\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Number of Support Vectors for each class:\", svm_clf.n_support_)\n",
    "print(\"Support Vectors:\\n\", svm_clf.support_vectors_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab3f4e",
   "metadata": {},
   "source": [
    "**Question 7:  Write a Python program to:** \n",
    "- Load the Breast Cancer dataset \n",
    "- Train a Gaussian Naïve Bayes model \n",
    "- Print its classification report including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e98b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naïve Bayes on Breast Cancer Dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      0.93      0.96        43\n",
      "      benign       0.96      1.00      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gaussian Naïve Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Gaussian Naïve Bayes on Breast Cancer Dataset\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e813b35",
   "metadata": {},
   "source": [
    "**Question 8: Write a Python program to:** \n",
    "- Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. \n",
    "- Print the best hyperparameters and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9105672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Wine Classifier Results:\n",
      "Best Parameters: {'C': 1, 'gamma': 0.01}\n",
      "Best Cross-Validation Score: 0.9788\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grid for C and gamma\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Create SVM classifier\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Test the best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"SVM Wine Classifier Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5920a84",
   "metadata": {},
   "source": [
    "**Question 9: Write a Python program to:** \n",
    "- Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). \n",
    "- Print the model's ROC-AUC score for its predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afe6790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Text Classifier Results:\n",
      "Dataset: 20newsgroups (alt.atheism vs soc.religion.christian)\n",
      "Training samples: 1079\n",
      "Test samples: 717\n",
      "ROC-AUC Score: 0.9680\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# Load 20newsgroups dataset (subset for binary classification)\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Train Naïve Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = nb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"Naïve Bayes Text Classifier Results:\")\n",
    "print(f\"Dataset: 20newsgroups ({categories[0]} vs {categories[1]})\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5c057",
   "metadata": {},
   "source": [
    "**Question 10: Imagine you’re working as a data scientist for a company that handles email communications.**\n",
    "\n",
    "**Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:**\n",
    "- Text with diverse vocabulary \n",
    "- Potential class imbalance (far more legitimate emails than spam) \n",
    "- Some incomplete or missing data \n",
    "**Explain the approach you would take to:**\n",
    "- Preprocess the data (e.g. text vectorization, handling missing data) \n",
    "- Choose and justify an appropriate model (SVM vs. Naïve Bayes) \n",
    "- Address class imbalance \n",
    "- Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de2cb9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Data Preprocessing: The Foundation for Success\n",
    "\n",
    "Effective preprocessing is the most critical step. I would go beyond simple techniques to build a stronger system.\n",
    "\n",
    "* **Text Vectorization:** I would start with **TF-IDF**, as it's a proven baseline for text classification. However, for a better solution, I would also look into **word embeddings** like Word2Vec or GloVe. These models capture the meaning of words, which is important for spotting clever spam that uses disguised language or slang. For example, a spam email might say \"free monie\" instead of \"free money,\" and a word embedding model would place these words close together in a vector space.\n",
    "\n",
    "* **Handling Missing Data:** This involves more than basic imputation. I would create a new feature that shows when data is missing. For instance, if the sender's email address is missing or faulty, that could signal a fraudulent email. The model can then learn to use this `is_sender_missing` feature as a predictor.\n",
    "\n",
    "* **Feature Engineering:** I would extract more features from the emails. These could include:\n",
    "    * Number of links in the email\n",
    "    * Presence of \"all caps\" words\n",
    "    * Word count of the email body\n",
    "    * Character-level features (e.g., number of special characters)\n",
    "    * Whether the email contains common spam words like \"viagra,\" \"lottery,\" or \"unsubscribe.\"\n",
    "\n",
    "### 2. Model Selection: A Justified Decision\n",
    "\n",
    "I would still choose **Multinomial Naïve Bayes** for its speed and good performance on text data. However, I would also consider a **Linear SVM** as a strong alternative.\n",
    "\n",
    "* **Naïve Bayes:** This is my go-to choice for its speed and simplicity. It provides a strong baseline.\n",
    "* **Linear SVM:** This model excels at finding a clear decision boundary in high-dimensional spaces. It considers features more thoroughly than Naïve Bayes and can often achieve higher accuracy. Its main drawback is slightly longer training time, but for a one-time training task, it’s a small price to pay for potentially better results. My final choice would be an **ensemble of these two models** or the one that performs best after cross-validation.\n",
    "\n",
    "### 3. Addressing Class Imbalance: Beyond Simple Resampling\n",
    "\n",
    "Basic resampling can introduce bias. I would use a more sophisticated approach:\n",
    "\n",
    "* **SMOTE (Synthetic Minority Over-sampling Technique):** This is a good starting point for oversampling the minority class.\n",
    "* **Cost-Sensitive Learning:** Instead of only balancing the dataset, I would adjust the model's cost function. This means informing the model that a **False Negative** (spam classified as non-spam) is much more costly than a **False Positive** (non-spam classified as spam). This encourages the model to prioritize catching all spam, even at the risk of a few minor errors. Many machine learning libraries allow you to set class weights to accomplish this.\n",
    "\n",
    "### 4. Performance Evaluation: A Holistic View\n",
    "\n",
    "My evaluation would not depend on a single metric. I would use a combination of tools to get a full picture.\n",
    "\n",
    "* **Confusion Matrix:** This is crucial for understanding where the model is making mistakes.\n",
    "* **Precision and Recall:** I would focus heavily on these. **High recall** is essential to prevent harmful spam from reaching a user's inbox, while high precision is important to avoid accidentally deleting a legitimate email. Balancing these two is key.\n",
    "* **F1-Score:** The F1-score is still my main metric for balancing precision and recall.\n",
    "* **Area Under the ROC Curve (AUC-ROC):** This provides an overall measure of the model's performance and its ability to tell the two classes apart.\n",
    "\n",
    "### 5. Business Impact: Beyond the Obvious\n",
    "\n",
    "The business value goes beyond simple productivity gains.\n",
    "\n",
    "* **Reputation and Trust:** A reliable email service that effectively blocks spam builds a strong reputation and user trust. This is a critical competitive advantage.\n",
    "* **Legal and Regulatory Compliance:** Many industries have strict regulations about data security, like HIPAA in healthcare. A solid spam filter is a key part of a cybersecurity strategy to ensure compliance and avoid hefty fines.\n",
    "* **Actionable Insights:** By analyzing the features the model found most predictive, we can learn more about spam attacks. For example, if a specific country's language or a certain type of attachment strongly indicates spam, this information can help improve security policies.\n",
    "* **Cost Savings:** Reducing server load from not processing large amounts of spam leads to direct cost savings. Additionally, preventing a single phishing attack can save millions in damages, making the model a crucial part of the company's financial defenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
