{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5252e87",
   "metadata": {},
   "source": [
    "**Question 1:  What is Logistic Regression, and how does it differ from Linear Regression?**\n",
    "\n",
    "Logistic Regression is a statistical method used for binary classification problems—where the outcome is categorical and typically has two classes (e.g., yes/no, spam/not spam, pass/fail).\n",
    "\n",
    "Logistic Regression is a supervised machine learning algorithm used for classification problems. Unlike linear regression which predicts continuous values it predicts the probability that an input belongs to a specific class. It is used for binary classification where the output can be one of two possible categories such as Yes/No, True/False or 0/1. It uses sigmoid function to convert inputs into a probability value between 0 and 1.\n",
    "\n",
    "**Key Differences from Linear Regression**\n",
    "\n",
    "**Output Type**\n",
    "\n",
    " - Linear Regression: Predicts continuous numeric values (like house prices or temperature).\n",
    " - Logistic Regression: predicts probabilities between 0 and 1, which are then converted to categorical predictions.\n",
    "\n",
    "**Use Case**\n",
    "- Linear Regression: Used for regression tasks.\n",
    "- Logistic Regression: Used for classification tasks.\n",
    "\n",
    "**Prediction Function**\n",
    "- Linear Regression: Straight line i.e y = wX + b\n",
    "- Logistic Regression: Sigmoid function i.e y = 1 / (1 + e^-(wX + b))\n",
    "\n",
    "**Target Variable**\n",
    "- Linear Regression: Continuous (house price)\n",
    "- Logistic Regression: Binary (0 or 1, yes or no)\n",
    "\n",
    "**Loss Function**\n",
    "- Linear Regression: Mean Squared Error (MSE)\n",
    "- Logistic Regression: Binary Cross-Entropy (Log Loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3425282",
   "metadata": {},
   "source": [
    "**Question 2: Explain the role of the Sigmoid function in Logistic Regression.**\n",
    "\n",
    "The sigmoid function is the mathematical heart of logistic regression, serving as the crucial link between linear combinations of input features and probability predictions.The Sigmoid function is crucial in Logistic Regression as it transforms linear combinations of input features into probabilities, enabling binary classification.\n",
    "\n",
    "The Sigmoid function, mathematically defined as as:σ(z) = 1 / (1 + e^(-z)), is an S-shaped curve that maps any real-valued number into the range (0, 1). This property makes it particularly suitable for modeling probabilities, as the output can be interpreted as the likelihood of a given input belonging to a particular class.\n",
    "\n",
    "**Role in Logistic Regression**\n",
    "\n",
    "- **Bounded Output (0 to 1):**\n",
    "The sigmoid function naturally constrains all outputs between 0 and 1, making it perfect for representing probabilities. No matter how large or small the input z becomes, the output will always be a valid probability value.\n",
    "\n",
    "- **S-Shaped Curve:**\n",
    "The function creates a smooth S-shaped (sigmoidal) curve that transitions gradually from 0 to 1. This provides a natural decision boundary and avoids abrupt jumps that would be problematic for classification.\n",
    "\n",
    "- **Monotonic and Differentiable:**\n",
    "The function is always increasing and smooth everywhere, which ensures that larger input values always correspond to higher probabilities. The differentiability is crucial for optimization algorithms like gradient descent.\n",
    "\n",
    "- **Probability Mapping:**\n",
    "The sigmoid transforms the linear predictor z (which can range from -∞ to +∞) into a probability between 0 and 1. This probability represents the likelihood of belonging to the positive class.\n",
    "\n",
    "- **Decision Boundary:**\n",
    "When σ(z) = 0.5, we have z = 0, which creates a natural decision boundary. Values above 0.5 typically predict class 1, while values below predict class 0.\n",
    "\n",
    "- **Interpretability:**\n",
    "The sigmoid's output directly represents confidence in the prediction. A value near 0.9 indicates high confidence for class 1, while 0.1 indicates high confidence for class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a20f6",
   "metadata": {},
   "source": [
    "**Question 3: What is Regularization in Logistic Regression and why is it needed?**\n",
    "\n",
    "Regularization is a fundamental technique used in logistic regression to prevent overfitting by adding a penalty term to the cost function. This penalty term constrains the magnitude of the model parameters (weights), encouraging the model to find simpler solutions that generalize better to unseen data. The core idea is to balance the model's ability to fit the training data with its complexity, following the principle that simpler models are often better at generalizing.\n",
    "\n",
    "It does this by adding a penalty term to the objective function (also called the loss function or error function) that the model is trying to minimize.By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely. The penalty term is a hyperparameter that controls the strength of the regularization. A higher value for the penalty term leads to stronger regularization and a simpler model, while a lower value allows the model to be more complex.\n",
    "\n",
    "\n",
    "**Why is Regularization Needed?**  \n",
    "\n",
    "**Preventing overfitting** is the main reason for regularization. Without it, logistic regression can memorize training data, including noise. This leads to poor performance on new data. Regularization limits this flexibility. It forces the model to learn general patterns instead of specific training examples.  \n",
    "\n",
    "**Handling high-dimensional data** is important when features outnumber samples. In these situations, unregularized models can become unstable and prone to overfitting. Regularization keeps the parameter space in check, which makes the model more stable and trustworthy.  \n",
    "\n",
    "**Managing multicollinearity** helps with the issue of highly correlated features that cause unstable parameter estimates. Regularization stabilizes these estimates and stops individual parameters from becoming too large.  \n",
    "\n",
    "**Numerical stability** is maintained by keeping parameter values within reasonable limits. This prevents overflow errors and helps make optimization more stable. It ensures reliable computation throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a4795",
   "metadata": {},
   "source": [
    "**Question 4: What are some common evaluation metrics for classification models, and why are they important?**\n",
    "\n",
    "When building machine learning models, it’s important to understand how well they perform. Evaluation metrics help us to measure the effectiveness of our models.\n",
    "There is no universal best metric, choosing the right one depends on the problem, data distribution and business needs.To evaluate the performance of classification models, we use the following metrics:\n",
    "\n",
    "**1. Accuracy:**\n",
    "Accuracy is a fundamental metric used for evaluating the performance of a classification model. It tells us the proportion of correct predictions made by the model out of all predictions.\n",
    "\n",
    "\n",
    "**2. Precision**\n",
    "It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences.\n",
    "\n",
    "**3. Recall**\n",
    "Recall or Sensitivity measures how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (false negative) is more costly than false positives.\n",
    "\n",
    "**4. F1 Score**\n",
    "The F1 Score is the harmonic mean of precision and recall. It is useful when we need a balance between precision and recall as it combines both into a single number. A high F1 score means the model performs well on both metrics. Its range is [0,1].\n",
    "\n",
    "Lower recall and higher precision gives us great accuracy but then it misses a large number of instances. More the F1 score better will be performance. It can be expressed mathematically in this way:\n",
    "\n",
    "**5. Logarithmic Loss (Log Loss)**\n",
    "Log loss measures the uncertainty of the model’s predictions. It is calculated by penalizing the model for assigning low probabilities to the correct classes. This metric is used in multi-class classification and is helpful when we want to assess a model’s confidence in its predictions. If there are N  samples belonging to the M class, then we calculate the Log loss in this way:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba416c",
   "metadata": {},
   "source": [
    "**Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, \n",
    "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. \n",
    "(Use Dataset from sklearn package)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d4a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Results:\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Accuracy: 0.9667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "df.to_csv(\"iris.csv\", index=False)\n",
    "df = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "X_iris = df.drop('target', axis=1)\n",
    "y_iris = df['target']\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_iris = LogisticRegression(random_state=42, max_iter=200)\n",
    "lr_iris.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_iris = lr_iris.predict(X_test_iris)\n",
    "accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)\n",
    "\n",
    "print(f\"Iris Dataset Results:\")\n",
    "print(f\"Training set size: {len(X_train_iris)}\")\n",
    "print(f\"Test set size: {len(X_test_iris)}\")\n",
    "print(f\"Accuracy: {accuracy_iris:.4f}\")\n",
    "print(f\"Classification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91eab1",
   "metadata": {},
   "source": [
    "**Question 6: Write a Python program to train a Logistic Regression model using L2 \n",
    "regularization (Ridge) and print the model coefficients and accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients:\n",
      "mean radius: -0.5115\n",
      "mean texture: -0.5527\n",
      "mean perimeter: -0.4763\n",
      "mean area: -0.5411\n",
      "mean smoothness: -0.2125\n",
      "mean compactness: 0.6483\n",
      "mean concavity: -0.6021\n",
      "mean concave points: -0.7042\n",
      "mean symmetry: -0.1672\n",
      "mean fractal dimension: 0.1997\n",
      "radius error: -1.0830\n",
      "texture error: 0.2488\n",
      "perimeter error: -0.5443\n",
      "area error: -0.9291\n",
      "smoothness error: -0.1603\n",
      "compactness error: 0.6472\n",
      "concavity error: 0.1606\n",
      "concave points error: -0.4438\n",
      "symmetry error: 0.3605\n",
      "fractal dimension error: 0.4379\n",
      "worst radius: -0.9476\n",
      "worst texture: -1.2551\n",
      "worst perimeter: -0.7632\n",
      "worst area: -0.9478\n",
      "worst smoothness: -0.7466\n",
      "worst compactness: 0.0555\n",
      "worst concavity: -0.8232\n",
      "worst concave points: -0.9537\n",
      "worst symmetry: -0.9392\n",
      "worst fractal dimension: -0.1873\n",
      "\n",
      "Intercept: 0.3022075735370298\n",
      "\n",
      "Model Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression with L2 regularization\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output\n",
    "print(\"Model Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nIntercept:\", model.intercept_[0])\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bacc3f",
   "metadata": {},
   "source": [
    "**Question 7: Write a Python program to train a Logistic Regression model for multiclass \n",
    "classification using multi_class='ovr' and print the classification report. \n",
    "(Use Dataset from sklearn package)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496187e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        12\n",
      "     class_1       0.88      1.00      0.93        14\n",
      "     class_2       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.93      0.94        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Logistic Regression with One-vs-Rest strategy\n",
    "model = LogisticRegression(\n",
    "    multi_class='ovr',\n",
    "    solver='lbfgs',\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b5ed2",
   "metadata": {},
   "source": [
    "**Question 8: Write a Python program to apply GridSearchCV to tune C and penalty \n",
    "hyperparameters for Logistic Regression and print the best parameters and validation \n",
    "accuracy. \n",
    "(Use Dataset from sklearn package)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5479fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'logreg__C': 0.1, 'logreg__penalty': 'l2'}\n",
      "Best Cross-Validation Accuracy: 0.9862\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create pipeline (Scaling + Logistic Regression)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, solver='liblinear', multi_class='ovr'))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100],     # Regularization strength\n",
    "    'logreg__penalty': ['l1', 'l2']           # Penalty type\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7467e76",
   "metadata": {},
   "source": [
    "**Question 9: Write a Python program to standardize the features before training Logistic \n",
    "Regression and compare the model's accuracy with and without scaling. \n",
    "(Use Dataset from sklearn package)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a69afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data Science\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Scaling: 0.9444\n",
      "Accuracy with Scaling:    1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Logistic Regression without scaling\n",
    "model_no_scale = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "model_no_scale.fit(X_train, y_train)\n",
    "y_pred_no_scale = model_no_scale.predict(X_test)\n",
    "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    "\n",
    "# Logistic Regression with scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy without Scaling: {acc_no_scale:.4f}\")\n",
    "print(f\"Accuracy with Scaling:    {acc_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1a013",
   "metadata": {},
   "source": [
    "**Question 10: Imagine you are working at an e-commerce company that wants to \n",
    "predict which customers will respond to a marketing campaign. Given an imbalanced \n",
    "dataset (only 5% of customers respond), describe the approach you’d take to build a \n",
    "Logistic Regression model — including data handling, feature scaling, balancing \n",
    "classes, hyperparameter tuning, and evaluating the model for this real-world business \n",
    "use case.**\n",
    "\n",
    "Data Handling & Preprocessing  \n",
    "I would start by loading the dataset and doing some initial exploratory data analysis (EDA). This involves checking for missing values, understanding data types, and spotting any outliers. For categorical features, I would use techniques like one-hot encoding to change them into a numerical format that the model can use. Next, I would split the data into a training set and a testing set, usually in a 70/30 or 80/20 ratio. It's important to keep the original class distribution in the split, which can be achieved through a stratified split.  \n",
    "\n",
    "Feature Scaling  \n",
    "Logistic Regression models are sensitive to the size of features. So, I would scale all numerical features. Standardization (Z-score normalization) is a good option here since it adjusts the data to have a mean of 0 and a standard deviation of 1. This stops features with larger scales from overpowering the model's learning. The scaling parameters (mean and standard deviation) would be learned from the training data and then applied to both the training and testing sets to avoid data leakage.  \n",
    "\n",
    "Balancing Classes  \n",
    "Since only 5% of customers respond, the dataset is very imbalanced. Training a model on this data would likely cause it to mostly predict the majority class (non-responders), leading to poor performance on the minority class. To fix this, I would use a method like oversampling the minority class. A common approach is SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic samples of the minority class to help balance the dataset. I would only apply this to the training data so the testing data accurately reflects the real-world distribution.  \n",
    "\n",
    "Model Training & Hyperparameter Tuning  \n",
    "I would use a Logistic Regression model with a penalty to prevent overfitting. The two common types are L1 (Lasso) and L2 (Ridge) penalties. L1 can help with feature selection by driving some coefficients to zero, while L2 is better for reducing the impact of less important features. I would use cross-validation on the training data to adjust the hyperparameters, mainly the regularization strength (C parameter). A Grid Search or Randomized Search would help find the best 'C' and 'penalty' combination for optimal performance.  \n",
    "\n",
    "Model Evaluation  \n",
    "Given the imbalanced nature of the data, accuracy is not a reliable measure. Instead, I would focus on metrics that provide better insights for imbalanced datasets:  \n",
    "\n",
    "- **Precision**: The percentage of positive identifications that were correct. This ensures that when we predict a customer will respond, they actually do.  \n",
    "- **Recall**: The percentage of actual responders that were correctly identified. This is vital for capturing potential responders.  \n",
    "- **F1-Score**: The harmonic mean of precision and recall. It offers a single score that balances both metrics.  \n",
    "- **Area Under the Receiver Operating Characteristic Curve (AUC-ROC)**: This metric assesses the model's ability to tell apart the positive and negative classes. An AUC of 1.0 indicates a perfect model, while 0.5 indicates a random guess. A higher AUC is always better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
